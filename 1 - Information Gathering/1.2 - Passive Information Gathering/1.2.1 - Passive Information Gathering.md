Footprinting = It refers to identifying more important information, pertinent to a particular target.

Information we are looking for at this stage:
- IP Addresses
- Hidden Directories
- Names
- Email Addresses
- Phone Numbers
- Physical Addresses
- Web technologies being used

Process:
- Finding IP Addresses
	host (Linux command):
		if multiple IPs, (could be dealing w proxy/cloudflare/firewall)
		provides ipv4 addresses, ipv6 address, mail server addresses
- Finding hidden directories
	robots.txt (Webpage)
		used to prevent crawlers from indexing certain pages of the site, also helps in footprinting
	sitemap.xml (Webpage)
		Its a xml file used to provide search engines with an organizedx way of indexing a website, can be used to find information. (look out for categories on a wp site)
- Footprinting
	BuiltWith (Technology Profiler / Extension)
		Helps us find what CMS and other web technologies are running on a website.
		Also lists some of the subdomains associated with the domain.
	Wappalyzer (Technology Profiler)
		Does almost the same work as BuiltWith, provides more limits in free plan
	WhatWeb (Linux Command)
		Website scanner, similar to BuiltWith/Wappalyzer, but provides more comprehensive scans and outputs.
- Website Downloader
	HTTrack (Website Copier / Downloader)
		Helps you download a copy of the whole website locally, helps with code review.
		Installable with webhttrack package
		Fails on sites with cloudflare or other WAF/proxy etc.
- Recon
	Whois (Linux Command - protocol)
		Helps find who registered the website, and with whom, and NAME Servers, (if a website is using Cloudflare, it will list that as name servers.), domain registration and renewal dates
		If DNSSEC is enabled, the domain owner's information is hidden
	NetCraft (Passive Recon Service) - sitereport.netcraft.com
		Provides whois information. SSL/TLS certificate details (and some major ssl/tls vulns present), Name Servers, Client Side / Server Side Web Technologies / Frameworks / Operating System, Hosting Providers History, Web Trackers / Analytics, SSL expiry date, SPF, DMARC.
		Can be considered a collection of all the information we could find manually with the tools used prior to this (except robots/sitemap/httrack).
	DNSRecon (Passive Recon Tool)
		Doesn't include DNS Zone Transfers, making it a passive tool.
		Helps find information about A records(IPV4), AAAA records (IPV6), MX (Mail server) records, NS (Name Server) records, TXT Records, DNS Zone Transfer Information.
		Helps find how websites are configured to run.
		MX Records can help us find mail service being used. "zonetransfer.me example"
	DNS Dumpster (Similar to DNS Recon but better)
		Shows tons of what DNSRecon does, but includes more options.
		Can help us find what other websites are using the same DNS server.
		Lists what options are active or passive.
		May also help identify subdomains.
		hostname can be pointing to cloud provider / hosting provider.
		Incase of  Name Servers, 1.xxx means primary NS, 2.xxx means secondary NS
- WAF Fingerprinting
	WAF Recon with wafw00f (WAF Fingerprinting Tool)
		Sends a HTTP request and analyzes the response.
		wafw00f -l lists potential WAFs it can help you detect.
		Can hallucinate
		wafw00f -a helps make it more accurate.
- Subdomain Enumeration (Passive)
	Sublist3r
		Sublist3r has both active and passive recon capabilities, as it doesn't use subdomain bruteforcing unless explicitly prompted to.
		Looks up subdomains of given domain on multiple search engines/databases like netcraft, ask, bing, yahoo, dnsdumpter, virustotal etc.
		A VPN can be used to bypass ip blocking of your attackbox.
		Lower threads can prevent ip blocks from search engines like Google.
		May list subdomains that are no longer active.
- Google Dorking
		Helps find information pertinent to a target.
		We can find specific subdomains, files etc. associated with the target.
		`site:ine.com`
		`inurl:admin`
		`intitle:admin`
		`site:*.ine.com`
		`site:*.ine.com intitle:admin`
		`filetype:pdf`
		dork for directory listing: `intitle:index of`
		`cache:ine.com` for cached site data
		`inurl:auth_user_file.txt`
		`inurl:passwd.txt`
		`intitle:"index of" "credentials"`
		Reference: Google Hacking DB, by OffSec
- WebArchive
		Helps find old public snapshots of websites
- Email Enumeration
	theHarvester
		Similar to Sublist3r, It performs passive enumeration using OSINT, with active recon available when explicitly stated.
		Helps in looking up emails, names, subdomains, IPs and URLs.
		Some DBs/Search Engines may need using API keys
		Can look up information based on domain name or company name.
		Company name needs to be specified with the domain option as well.
		Specifying search engines/databases to use is required
		spyse search engine is worth checking out, but requires a paid subscription
- Leaked Passwords Enumeration
		Leaked passwords can be used to perform password spraying attacks, they can also be used directly login using the leaked credentials.
	haveibeenpwned?
		It aggregates leaked password databases and accepts queries using either email ids or phone numbers to provide a result that includes all the data breaches that the id or number has been found in.
		Has an API available.