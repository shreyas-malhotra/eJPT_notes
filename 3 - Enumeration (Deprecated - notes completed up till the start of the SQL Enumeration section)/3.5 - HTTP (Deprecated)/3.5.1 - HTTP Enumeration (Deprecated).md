- HTTP is a protocol used for hosting websites.
- HTTP can generally be found on both internal and external network scans.
- The attack surface on websites/webservers is the most prominent relative to anything else on a given network.
- The preliminary enumeration process for HTTP involves opening the website and analyzing it via a web browser.

IIS and Apache
	- IIS (Internet Information Services) is a web server software package designed for Windows Server. It provides a secure and manageable platform for hosting websites, web applications, and services. IIS supports various protocols like HTTP, HTTPS, FTP, FTPS, SMTP, and more, allowing it to serve web content and handle web traffic efficiently.
	- Apache is the Linux counterpart of IIS, most of the difference during a penetration test with these two would be the way someone interacts with the machine post exploitation, and nothing more.

HTTP Enumeration
	- Service version detection using Nmap
		- `nmap -sV 10.10.0.1`
	- Using WhatWeb to further enumerate the site:
		- WhatWeb recognizes web technologies including content management systems (CMS), blogging platforms, statistic/analytics packages, JavaScript libraries, web servers, and embedded devices.  
		- WhatWeb has over 1800 plugins, each to recognize something different. WhatWeb also identifies version numbers, email addresses, account IDs, web framework modules, SQL errors, and more.
		- `whatweb 10.10.0.1`
	- Banner Grabbing using http requests
		- `http 10.10.0.1`
	- Banner Grabbing using Nmap
		- Example: `sudo nmap -p80 192.168.0.1 -script banner`
	- HTTP Enumeration using cURL
		- cURL, which stands for client URL, is a command line tool that developers use to transfer data to and from a server.
		- At the most fundamental, cURL lets you talk to a server by specifying the location (in the form of a URL) and the data you want to send.
		- cURL supports several different protocols, including HTTP and HTTPS, and runs on almost every platform.
		- This makes cURL ideal for testing communication from almost any device (as long as it has a command line and network connectivity) from a local server to most edge devices.
		- Example: `curl 192.168.0.1`
	- HTTP Enumeration using Wget
		- Wget is a free software package for retrieving files using HTTP, HTTPS, FTP and FTPS, the most widely used Internet protocols.
		- Example: `wget http://192.168.0.1/index`
	- HTTP Enumeration using lynx
		- Lynx is a text web browser.
		- Example: `lynx http://192.168.0.1/`
	- Website enumeration with browsh
		- Browsh is a fully-modern text-based browser. It renders anything that a modern browser can; HTML5, CSS3, JS, video and even WebGL.
		- Essentially, it renders websites in a TUI format on the command line.
		- The use case for browsh is to get an idea of a website's render on a command-line based system.
		- `browsh --startup-url 10.10.0.1`
	- HTTP version enumeration using Metasploit Framework
		- `msfconsole`
		- `search http_version`
		- `use 0`
		- `set RHOSTS 192.168.0.1`
		- `options`
		- In case enumeration is being performed on port 443, set the SSL option to true.
		- `run`
	- HTTP directory bruteforcing using Metasploit Framework
		- `msfconsole`
		- `use auxiliary/scanner/http/brute_dirs`
		- `set RHOSTS 192.168.0.1`
		- `run`
		- `exit`
		- The wordlist for directory bruteforcing is included in Metasploit Framework by default: `/usr/share/metasploit-framework/wordlists/directory.txt`
	 - Web content enumeration with dirb
		- DIRB is a Web Content Scanner. It looks for existing (and/or hidden) Web Objects. It basically works by launching a dictionary based attack against a web server and analyzing the responses.
		- DIRB comes with a set of preconfigured attack wordlists for easy usage but you can use your custom wordlists. Also DIRB sometimes can be used as a classic CGI scanner, but remember that it is a content scanner not a vulnerability scanner.
		- Example: `dirb http://10.10.0.1/`
	- robots.txt
		- robots.txt is a file hosted on the web server for the search engine to understand what files the site developers don't want to be indexed in search results.
		- robots.txt also has configuration options to specify what search engine crawling bots are permitted to crawl the sites for search engine indexing.
		- robots.txt enumeration using Metasploit Framework
			- `msfconsole`
			- `use auxiliary/scanner/http/robots_txt`
			- `set RHOSTS 192.168.0.1`
			- `show options`
			- `run`
		- Or just access robots.txt on the target site using a web browser.